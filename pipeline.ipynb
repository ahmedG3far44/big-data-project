{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01ced89e-45e4-43da-8289-817174e1fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "from botocore.client import Config\n",
    "from datetime import datetime, timedelta\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c1bbd85-a345-4fbf-8396-c7dad397619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting data generation for 5000 rows...\n",
      "INFO: Data generation complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame Summary ---\n",
      "Total Rows: 5000\n",
      "Data Types:\n",
      "weather_id              Int64\n",
      "temperature_c         float64\n",
      "date_time              object\n",
      "humidity                Int64\n",
      "rain_mm               float64\n",
      "season               category\n",
      "wind_speed_kmh        float64\n",
      "visibility_m           object\n",
      "weather_condition    category\n",
      "air_pressure_hpa      float64\n",
      "city                   object\n",
      "dtype: object\n",
      "\n",
      "First 5 Rows (Note the various formats/NaNs):\n",
      "   weather_id  temperature_c            date_time  humidity  rain_mm  season  \\\n",
      "0        5001           6.09  2024-01-27 20:01:00        43     1.55  Winter   \n",
      "1        5002          -1.68               25/:61        45    46.56  Winter   \n",
      "2        5003          10.68  2022-12-11 07:05:00        55    35.32  Winter   \n",
      "3        5004          13.73  2024-05-07 02:38:00        35    25.19  Spring   \n",
      "4        5005           3.46  2022-02-04 09:32:00        65    23.74     NaN   \n",
      "\n",
      "   wind_speed_kmh visibility_m weather_condition  air_pressure_hpa    city  \n",
      "0           20.03         5401             Clear           1000.94  London  \n",
      "1           19.31         4285               Fog            954.14  London  \n",
      "2           52.82         3271             Clear           1012.75  London  \n",
      "3            3.00         4407              Rain           1027.68  London  \n",
      "4           68.26         9113             Clear            964.24  London  \n",
      "\n",
      "Value Counts for a Messy Column (date_time):\n",
      "date_time\n",
      "unknown                50\n",
      "nan                    50\n",
      "25/:61                 50\n",
      "2099-13-40             48\n",
      "2023-04-26 10:39:00     2\n",
      "2024-12-05 17:54:00     2\n",
      "2022-11-11 00:30:00     2\n",
      "2023-11-08 16:28:00     2\n",
      "2024-08-23 13:11:00     1\n",
      "2022-07-07 07:53:00     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for a Messy Column (humidity, showing NaNs and potential outliers):\n",
      "humidity\n",
      "-10    14\n",
      "20     59\n",
      "21     61\n",
      "22     56\n",
      "23     49\n",
      "Name: count, dtype: Int64\n",
      "humidity\n",
      "150    11\n",
      "100    70\n",
      "99     51\n",
      "98     48\n",
      "97     61\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Value Counts for weather_id (showing duplicates and NaNs):\n",
      "weather_id\n",
      "<NA>    25\n",
      "5217     6\n",
      "5355     6\n",
      "5275     4\n",
      "5209     4\n",
      "5945     4\n",
      "5977     4\n",
      "5262     4\n",
      "5220     4\n",
      "5573     3\n",
      "Name: count, dtype: Int64\n",
      "DataFrame saved to weather_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "def generate_weather_data(n_rows: int = 5000, city: str = 'London') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a synthetic weather data DataFrame of a specified size, incorporating\n",
    "    various data quality issues (NULLs, duplicates, outliers, bad formats)\n",
    "    based on the provided schema description.\n",
    "\n",
    "    Args:\n",
    "        n_rows (int): The number of data rows to generate (e.g., 5000, 6000, 7000).\n",
    "        city (str): The primary city name for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the synthetic, messy data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting data generation for {n_rows} rows...\")\n",
    "    data = {}\n",
    "\n",
    "    # --- 2. Generate Base Dates (Used for dependency generation) ---\n",
    "    start_date = datetime(2022, 1, 1)\n",
    "    end_date = datetime(2024, 12, 31)\n",
    "    time_range_days = (end_date - start_date).days\n",
    "\n",
    "    # Generate random days within the time range for non-uniform distribution\n",
    "    random_days = np.random.randint(0, time_range_days, n_rows)\n",
    "    base_dates = np.array([\n",
    "        start_date + timedelta(\n",
    "            days=int(d),\n",
    "            hours=np.random.randint(0, 23),\n",
    "            minutes=np.random.randint(0, 59)\n",
    "        ) for d in random_days\n",
    "    ])\n",
    "\n",
    "    # --- 4. Get Clean Seasons (Used for Temperature dependency) ---\n",
    "    def get_clean_season(date: datetime) -> str:\n",
    "        \"\"\"Determines season based on a simple month rule.\"\"\"\n",
    "        if 3 <= date.month <= 5: return 'Spring'\n",
    "        if 6 <= date.month <= 8: return 'Summer'\n",
    "        if 9 <= date.month <= 11: return 'Autumn'\n",
    "        return 'Winter'\n",
    "\n",
    "    seasons_clean = np.array([get_clean_season(d) for d in base_dates])\n",
    "\n",
    "    # --- 5. temperature_c (Float) - Dependency on Season ---\n",
    "    temperatures = np.zeros(n_rows)\n",
    "    for s in ['Winter', 'Spring', 'Summer', 'Autumn']:\n",
    "        indices = np.where(seasons_clean == s)[0]\n",
    "\n",
    "        if s == 'Winter': # Range -5 to 15\n",
    "            temps = np.random.uniform(-5, 15, len(indices))\n",
    "        elif s == 'Spring': # Range 5 to 20\n",
    "            temps = np.random.uniform(5, 20, len(indices))\n",
    "        elif s == 'Summer': # Range 15 to 35\n",
    "            temps = np.random.uniform(15, 35, len(indices))\n",
    "        else: # Autumn, Range 0 to 25\n",
    "            temps = np.random.uniform(0, 25, len(indices))\n",
    "\n",
    "        temperatures[indices] = temps\n",
    "\n",
    "    # Introduce Outliers (-30, 60)\n",
    "    outlier_count = int(n_rows * 0.005)\n",
    "    temperatures[np.random.choice(n_rows, outlier_count, replace=False)] = np.random.choice([-30.0, 60.0], outlier_count)\n",
    "\n",
    "    # Introduce NULLs\n",
    "    temperatures[np.random.choice(n_rows, int(n_rows * 0.02), replace=False)] = np.nan\n",
    "    data['temperature_c'] = temperatures.round(2)\n",
    "\n",
    "    # --- 6. humidity (Integer %) ---\n",
    "    humidity = np.random.randint(20, 101, n_rows)\n",
    "    # Introduce Outliers (-10, 150)\n",
    "    outlier_count = int(n_rows * 0.005)\n",
    "    humidity_outliers = np.random.choice([-10, 150], outlier_count)\n",
    "    humidity_indices = np.random.choice(n_rows, outlier_count, replace=False)\n",
    "    humidity[humidity_indices] = humidity_outliers\n",
    "\n",
    "    # Introduce NULLs\n",
    "    humidity_nulls = np.random.choice(n_rows, int(n_rows * 0.02), replace=False)\n",
    "    humidity = humidity.astype(float) # Convert to float to hold NaN before final cast\n",
    "    data['humidity'] = humidity\n",
    "\n",
    "    # --- 7. rain_mm (Float) ---\n",
    "    rain = np.random.rand(n_rows) * 50 # Typical 0-50mm\n",
    "    # Introduce Extreme values (120+)\n",
    "    extreme_count = int(n_rows * 0.002)\n",
    "    rain[np.random.choice(n_rows, extreme_count, replace=False)] = np.random.uniform(120, 150, extreme_count)\n",
    "    # Introduce NULLs\n",
    "    rain[np.random.choice(n_rows, int(n_rows * 0.02), replace=False)] = np.nan\n",
    "    data['rain_mm'] = rain.round(2)\n",
    "\n",
    "    # --- 8. wind_speed_kmh (Float) ---\n",
    "    wind_speed = np.random.rand(n_rows) * 80 # Typical 0-80 km/h\n",
    "    # Introduce Outliers (200+);\n",
    "    outlier_count = int(n_rows * 0.002)\n",
    "    wind_speed[np.random.choice(n_rows, outlier_count, replace=False)] = np.random.uniform(200, 250, outlier_count)\n",
    "    # Introduce NULLs\n",
    "    wind_speed[np.random.choice(n_rows, int(n_rows * 0.02), replace=False)] = np.nan\n",
    "    data['wind_speed_kmh'] = wind_speed.round(2)\n",
    "\n",
    "    # --- 9. visibility_m (Integer/Messy) ---\n",
    "    visibility_base = np.random.randint(50, 10001, n_rows)\n",
    "    visibility_messy = visibility_base.astype(object)\n",
    "\n",
    "    # Extreme values (50,000)\n",
    "    visibility_messy[np.random.choice(n_rows, int(n_rows * 0.001), replace=False)] = 50000\n",
    "    # NULLs\n",
    "    visibility_messy[np.random.choice(n_rows, int(n_rows * 0.02), replace=False)] = np.nan\n",
    "    # Non-numeric strings (small fraction)\n",
    "    non_numeric_indices = np.random.choice(n_rows, int(n_rows * 0.01), replace=False)\n",
    "    visibility_messy[non_numeric_indices] = np.random.choice(['low', 'unknown', 'N/A', 'high'], len(non_numeric_indices))\n",
    "\n",
    "    data['visibility_m'] = visibility_messy\n",
    "\n",
    "    # --- 10. weather_condition (Category) ---\n",
    "    conditions = ['Clear', 'Rain', 'Fog', 'Storm', 'Snow']\n",
    "    weather_condition = np.random.choice(conditions, n_rows, p=[0.4, 0.2, 0.15, 0.05, 0.2])\n",
    "    # Introduce NULLs\n",
    "    weather_condition[np.random.choice(n_rows, int(n_rows * 0.03), replace=False)] = np.nan\n",
    "    # Remove .astype('category') here, will apply after DataFrame creation\n",
    "    data['weather_condition'] = weather_condition\n",
    "\n",
    "    # --- 11. air_pressure_hpa (Float) ---\n",
    "    air_pressure = np.random.uniform(950, 1050, n_rows)\n",
    "    # Introduce NULLs\n",
    "    air_pressure[np.random.choice(n_rows, int(n_rows * 0.02), replace=False)] = np.nan\n",
    "    data['air_pressure_hpa'] = air_pressure.round(2)\n",
    "\n",
    "    # --- 3. city (String) ---\n",
    "    city_data = np.full(n_rows, city, dtype=object)\n",
    "    # Introduce NULLs/unknown\n",
    "    city_data[np.random.choice(n_rows, int(n_rows * 0.02), replace=False)] = np.nan\n",
    "    city_data[np.random.choice(n_rows, int(n_rows * 0.01), replace=False)] = 'unknown'\n",
    "    data['city'] = city_data\n",
    "\n",
    "    # --- Create DataFrame and Dtype Casts ---\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Now, convert 'weather_condition' to category type in the DataFrame\n",
    "    df['weather_condition'] = df['weather_condition'].astype('category')\n",
    "\n",
    "    # --- 2. date_time (Messy Scenarios) ---\n",
    "    date_times_messy = base_dates.astype(str)\n",
    "\n",
    "    # Apply format variations\n",
    "    indices_v1 = np.random.choice(n_rows, int(n_rows * 0.1), replace=False) # 10%\n",
    "    date_times_messy[indices_v1] = [d.strftime('%d/%m/%Y %H:%M') for d in base_dates[indices_v1]]\n",
    "\n",
    "    indices_v2 = np.random.choice(n_rows, int(n_rows * 0.1), replace=False) # 10%\n",
    "    date_times_messy[indices_v2] = [d.strftime('%Y-%m-%dT%H:%M') for d in base_dates[indices_v2]]\n",
    "\n",
    "    # Invalid / Garbage\n",
    "    date_times_messy[np.random.choice(n_rows, int(n_rows * 0.01), replace=False)] = '25/:61'\n",
    "    date_times_messy[np.random.choice(n_rows, int(n_rows * 0.01), replace=False)] = '2099-13-40'\n",
    "\n",
    "    # NULLs/unknown\n",
    "    date_times_messy[np.random.choice(n_rows, int(n_rows * 0.01), replace=False)] = np.nan\n",
    "    date_times_messy[np.random.choice(n_rows, int(n_rows * 0.01), replace=False)] = 'unknown'\n",
    "\n",
    "    df.insert(1, 'date_time', date_times_messy)\n",
    "\n",
    "    # --- 4. season (Messy Scenarios) ---\n",
    "    # Season derivation is based on base_dates, now introduce additional NULLs\n",
    "    seasons_messy = seasons_clean.astype(object)\n",
    "    seasons_messy[np.random.choice(n_rows, int(n_rows * 0.03), replace=False)] = np.nan\n",
    "    df.insert(4, 'season', seasons_messy) # Insert as object first\n",
    "    df['season'] = df['season'].astype('category') # Then convert the DataFrame column\n",
    "\n",
    "    # --- 1. weather_id (Integer) ---\n",
    "    base_ids = np.arange(5001, 5001 + n_rows)\n",
    "    weather_ids = base_ids.astype(float)\n",
    "\n",
    "    # Duplicates (10% of rows reuse an ID from the first 1000)\n",
    "    duplicate_indices = np.random.choice(n_rows, int(n_rows * 0.1), replace=False)\n",
    "    weather_ids[duplicate_indices] = np.random.choice(base_ids[:1000], len(duplicate_indices))\n",
    "\n",
    "    # NULLs (missing ID)\n",
    "    weather_ids[np.random.choice(n_rows, int(n_rows * 0.005), replace=False)] = np.nan\n",
    "\n",
    "    # Int64 allows NA (pandas integer type for nullable columns)\n",
    "    df.insert(0, 'weather_id', pd.Series(weather_ids).astype(pd.Int64Dtype()))\n",
    "\n",
    "    # --- Final Dtype Casting for correct NA handling ---\n",
    "    df['humidity'] = df['humidity'].astype(pd.Int64Dtype())\n",
    "    # visibility_m is left as object/string due to non-numeric strings\n",
    "\n",
    "    logging.info(\"Data generation complete.\")\n",
    "    return df\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Use the function to generate a DataFrame with the requested size (e.g., 7000 rows)\n",
    "DATA_SIZE = 5000\n",
    "weather_df = generate_weather_data(n_rows=DATA_SIZE)\n",
    "\n",
    "# Print a summary to verify the generation and messiness\n",
    "print(\"\\n--- DataFrame Summary ---\")\n",
    "print(f\"Total Rows: {weather_df.shape[0]}\")\n",
    "print(f\"Data Types:\\n{weather_df.dtypes}\")\n",
    "print(\"\\nFirst 5 Rows (Note the various formats/NaNs):\")\n",
    "print(weather_df.head())\n",
    "print(\"\\nValue Counts for a Messy Column (date_time):\")\n",
    "# Show the top non-null values to see format variations\n",
    "print(weather_df['date_time'].value_counts(dropna=False).head(10))\n",
    "\n",
    "# Example of identifying messiness in humidity (will show outliers and NaNs)\n",
    "print(\"\\nValue Counts for a Messy Column (humidity, showing NaNs and potential outliers):\")\n",
    "print(weather_df['humidity'].value_counts(dropna=False).sort_index().head(5))\n",
    "print(weather_df['humidity'].value_counts(dropna=False).sort_index(ascending=False).head(5))\n",
    "\n",
    "print(\"\\nValue Counts for weather_id (showing duplicates and NaNs):\")\n",
    "print(weather_df['weather_id'].value_counts(dropna=False).head(10))\n",
    "\n",
    "weather_df.to_csv('weather_data.csv', index=False)\n",
    "print(\"DataFrame saved to weather_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a327a4ee-b150-43f8-af6b-84765c1ca1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating Traffic Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2882802-b282-4a84-a84a-81b0303f9b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting traffic data generation for 5000 rows...\n",
      "INFO: Data generation complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Exported ---\n",
      "DataFrame successfully saved to traffic_data.csv\n",
      "\n",
      "--- DataFrame Summary ---\n",
      "Total Rows: 5000\n",
      "Data Types:\n",
      "traffic_id             Int64\n",
      "area                  object\n",
      "date_time             object\n",
      "vehicle_count          Int64\n",
      "avg_speed_kmh        float64\n",
      "accident_count         Int64\n",
      "congestion_level    category\n",
      "road_condition      category\n",
      "visibility_m           Int64\n",
      "city                  object\n",
      "dtype: object\n",
      "\n",
      "First 5 Rows (Note the various formats/NaNs):\n",
      "   traffic_id        area            date_time  vehicle_count  avg_speed_kmh  \\\n",
      "0        9001   Islington  2023-03-03 18:59:09              8          95.42   \n",
      "1        9002  Kensington  2024-10-15 07:48:09            294          74.04   \n",
      "2        9003  Kensington  2023-07-13 22:33:25           2694          98.45   \n",
      "3        9004     Chelsea  2024-03-07 19:27:35           1974          95.09   \n",
      "4        9005  Kensington  2024-05-30T09:31:00           2995          40.61   \n",
      "\n",
      "   accident_count congestion_level road_condition  visibility_m    city  \n",
      "0               1              Low            Dry          6616  London  \n",
      "1               0              Low            Wet          6996  London  \n",
      "2               2           Medium            Dry          8316  London  \n",
      "3               0              NaN            Dry          9773  London  \n",
      "4               0           Medium            Wet          7549  London  \n",
      "\n",
      "Value Counts for a Messy Column (date_time, showing format variations):\n",
      "date_time\n",
      "nan                    50\n",
      "TBD                    25\n",
      "2099-00-00 99:99       25\n",
      "2023-05-22 22:25:29     2\n",
      "2023-07-20 11:25:41     1\n",
      "2023-05-16 01:46:29     1\n",
      "27/06/2024 06:55        1\n",
      "2023-12-05 08:44:54     1\n",
      "2024-02-07 20:51:24     1\n",
      "2024-10-15 15:26:05     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for a Messy Column (vehicle_count, showing NaNs and potential outliers):\n",
      "vehicle_count\n",
      "27899      1\n",
      "28431      1\n",
      "28941      1\n",
      "29067      1\n",
      "<NA>     100\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Value Counts for traffic_id (showing duplicates and NaNs):\n",
      "traffic_id\n",
      "<NA>    25\n",
      "9083     6\n",
      "9764     5\n",
      "9642     5\n",
      "9913     4\n",
      "9618     4\n",
      "9002     4\n",
      "9446     4\n",
      "9941     4\n",
      "9215     4\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging for non-critical information\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "def generate_traffic_data(n_rows: int = 5000, city: str = 'London') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a synthetic traffic data DataFrame, incorporating various data quality\n",
    "    issues (NULLs, duplicates, outliers, bad formats) based on the provided schema.\n",
    "\n",
    "    Args:\n",
    "        n_rows (int): The number of data rows to generate.\n",
    "        city (str): The primary city name for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the synthetic, messy data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting traffic data generation for {n_rows} rows...\")\n",
    "    data = {}\n",
    "\n",
    "    # --- 1. Base Dates (Used for dependency generation) ---\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2024, 12, 31)\n",
    "    time_range_seconds = int((end_date - start_date).total_seconds())\n",
    "\n",
    "    # Generate random seconds within the time range\n",
    "    random_seconds = np.random.randint(0, time_range_seconds, n_rows)\n",
    "    # Ensure times are rounded to the nearest minute/hour for traffic measurement consistency\n",
    "    base_dates = np.array([\n",
    "        start_date + timedelta(seconds=int(s)) for s in random_seconds # Cast s to int\n",
    "    ])\n",
    "\n",
    "    # --- 4. area (String) ---\n",
    "    # Used as a dependency for vehicle count/speed (e.g., Chelsea might be slower than Southwark)\n",
    "    areas = ['Camden', 'Chelsea', 'Islington', 'Southwark', 'Kensington', 'Westminster']\n",
    "    area_weights = [0.15, 0.20, 0.15, 0.15, 0.25, 0.10] # Weights to simulate varying sample rates per area\n",
    "\n",
    "    clean_areas = np.random.choice(areas, n_rows, p=area_weights)\n",
    "    area_data = clean_areas.astype(object)\n",
    "\n",
    "    # Introduce NULL area values\n",
    "    area_data[np.random.choice(n_rows, int(n_rows * 0.02), replace=False)] = np.nan\n",
    "    data['area'] = area_data\n",
    "\n",
    "    # --- 5. vehicle_count (Integer) - Dependency on Area ---\n",
    "    vehicle_counts = np.zeros(n_rows)\n",
    "    for area in areas:\n",
    "        indices = np.where(clean_areas == area)[0]\n",
    "\n",
    "        # Chelsea/Westminster (Central/Dense) tend to have higher vehicle counts\n",
    "        if area in ['Chelsea', 'Westminster', 'Kensington']:\n",
    "            counts = np.random.randint(100, 5001, len(indices))\n",
    "        else: # Other areas\n",
    "            counts = np.random.randint(0, 3001, len(indices))\n",
    "\n",
    "        vehicle_counts[indices] = counts\n",
    "\n",
    "    # Introduce Outliers (20,000+)\n",
    "    outlier_count = int(n_rows * 0.003)\n",
    "    vehicle_counts[np.random.choice(n_rows, outlier_count, replace=False)] = np.random.randint(20000, 30000, outlier_count)\n",
    "\n",
    "    # Introduce NULLs\n",
    "    vehicle_counts_nulls = np.random.choice(n_rows, int(n_rows * 0.02), replace=False)\n",
    "    # Convert to float temporarily to hold NaN\n",
    "    vehicle_counts = vehicle_counts.astype(float)\n",
    "    vehicle_counts[vehicle_counts_nulls] = np.nan\n",
    "    data['vehicle_count'] = vehicle_counts\n",
    "\n",
    "    # --- 6. avg_speed_kmh (Float) - Dependency on Vehicle Count (Negative correlation) ---\n",
    "    # Base speed for low traffic: 60-120 km/h (highway context)\n",
    "    # Base speed for high traffic: 3-50 km/h (city context)\n",
    "    avg_speeds = np.random.uniform(3, 120, n_rows)\n",
    "\n",
    "    # Apply negative correlation: High count = Low speed\n",
    "    high_traffic_indices = np.where(vehicle_counts > 3000)[0]\n",
    "    avg_speeds[high_traffic_indices] = np.random.uniform(3, 30, len(high_traffic_indices))\n",
    "\n",
    "    # Apply positive correlation: Low count = High speed\n",
    "    low_traffic_indices = np.where(vehicle_counts < 500)[0]\n",
    "    avg_speeds[low_traffic_indices] = np.random.uniform(40, 100, len(low_traffic_indices))\n",
    "\n",
    "    # Introduce Invalid values (negative speeds)\n",
    "    invalid_count = int(n_rows * 0.005)\n",
    "    avg_speeds[np.random.choice(n_rows, invalid_count, replace=False)] = np.random.uniform(-10.0, -1.0, invalid_count)\n",
    "\n",
    "    # Introduce NULLs\n",
    "    avg_speeds[np.random.choice(n_rows, int(n_rows * 0.02), replace=False)] = np.nan\n",
    "    data['avg_speed_kmh'] = avg_speeds.round(2)\n",
    "\n",
    "    # --- 7. accident_count (Integer) - Dependency on Congestion and Speed ---\n",
    "    accident_counts = np.random.poisson(lam=1, size=n_rows) # Most are 0 or 1\n",
    "    accident_counts[accident_counts > 10] = 10 # Cap at 10 for 'expected' range\n",
    "\n",
    "    # High Congestion/Low Speed -> Higher accident risk (set to 5 or less)\n",
    "    high_risk_indices = np.where((vehicle_counts > 4000) | (avg_speeds < 10))[0]\n",
    "    accident_counts[high_risk_indices] = np.random.poisson(lam=2, size=len(high_risk_indices))\n",
    "\n",
    "    # Introduce Extreme values (50+)\n",
    "    extreme_count = int(n_rows * 0.001)\n",
    "    accident_counts[np.random.choice(n_rows, extreme_count, replace=False)] = np.random.randint(50, 70, extreme_count)\n",
    "\n",
    "    # Introduce NULLs\n",
    "    accident_counts_nulls = np.random.choice(n_rows, int(n_rows * 0.02), replace=False)\n",
    "    accident_counts = accident_counts.astype(float)\n",
    "    accident_counts[accident_counts_nulls] = np.nan\n",
    "    data['accident_count'] = accident_counts\n",
    "\n",
    "    # --- 8. congestion_level (Category) - Dependency on Vehicle Count/Speed ---\n",
    "    congestion = np.full(n_rows, 'Medium', dtype=object)\n",
    "\n",
    "    # High Congestion (High count, Low speed)\n",
    "    high_indices = np.where((vehicle_counts > 4500) | (avg_speeds < 15))[0]\n",
    "    congestion[high_indices] = 'High'\n",
    "\n",
    "    # Low Congestion (Low count, High speed)\n",
    "    low_indices = np.where((vehicle_counts < 1000) & (avg_speeds > 60))[0]\n",
    "    congestion[low_indices] = 'Low'\n",
    "\n",
    "    # Introduce NULLs\n",
    "    congestion[np.random.choice(n_rows, int(n_rows * 0.03), replace=False)] = np.nan\n",
    "    data['congestion_level'] = congestion # Assign numpy array directly\n",
    "\n",
    "    # --- 9. road_condition (Category) ---\n",
    "    conditions = ['Dry', 'Wet', 'Snowy', 'Damaged']\n",
    "    # Wet/Dry are most common\n",
    "    road_condition = np.random.choice(conditions, n_rows, p=[0.7, 0.2, 0.05, 0.05])\n",
    "\n",
    "    # Introduce NULLs\n",
    "    road_condition[np.random.choice(n_rows, int(n_rows * 0.03), replace=False)] = np.nan\n",
    "    data['road_condition'] = road_condition # Assign numpy array directly\n",
    "\n",
    "    # --- 10. visibility_m (Integer) ---\n",
    "    visibility = np.random.randint(50, 10001, n_rows) # Typical: 50-10,000\n",
    "\n",
    "    # Introduce NULLs\n",
    "    visibility_nulls = np.random.choice(n_rows, int(n_rows * 0.02), replace=False)\n",
    "    visibility = visibility.astype(float)\n",
    "    visibility[visibility_nulls] = np.nan\n",
    "    data['visibility_m'] = visibility\n",
    "\n",
    "    # --- 3. city (String) ---\n",
    "    city_data = np.full(n_rows, city, dtype=object)\n",
    "    # Introduce NULLs\n",
    "    city_data[np.random.choice(n_rows, int(n_rows * 0.01), replace=False)] = np.nan\n",
    "    data['city'] = city_data\n",
    "\n",
    "    # --- Create DataFrame and Dtype Casts ---\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Convert to category Dtype after DataFrame creation\n",
    "    df['congestion_level'] = df['congestion_level'].astype('category')\n",
    "    df['road_condition'] = df['road_condition'].astype('category')\n",
    "\n",
    "    # --- 2. date_time (Messy Scenarios) ---\n",
    "    date_times_messy = base_dates.astype(str)\n",
    "\n",
    "    # Apply format variations (10% of rows for each major variation)\n",
    "    indices_v1 = np.random.choice(n_rows, int(n_rows * 0.1), replace=False)\n",
    "    date_times_messy[indices_v1] = [d.strftime('%d/%m/%Y %H:%M') for d in base_dates[indices_v1]]\n",
    "\n",
    "    indices_v2 = np.random.choice(n_rows, int(n_rows * 0.1), replace=False)\n",
    "    date_times_messy[indices_v2] = [d.strftime('%Y-%m-%dT%H:%M:00') for d in base_dates[indices_v2]] # Extra seconds zeroed\n",
    "\n",
    "    # Invalid / Garbage\n",
    "    date_times_messy[np.random.choice(n_rows, int(n_rows * 0.005), replace=False)] = 'TBD'\n",
    "    date_times_messy[np.random.choice(n_rows, int(n_rows * 0.005), replace=False)] = '2099-00-00 99:99'\n",
    "\n",
    "    # NULLs\n",
    "    date_times_messy[np.random.choice(n_rows, int(n_rows * 0.01), replace=False)] = np.nan\n",
    "\n",
    "    df.insert(1, 'date_time', date_times_messy)\n",
    "\n",
    "    # --- 1. traffic_id (Integer) ---\n",
    "    base_ids = np.arange(9001, 9001 + n_rows)\n",
    "    traffic_ids = base_ids.astype(float)\n",
    "\n",
    "    # Duplicates (10% of rows reuse an ID from the first 1000)\n",
    "    duplicate_indices = np.random.choice(n_rows, int(n_rows * 0.1), replace=False)\n",
    "    traffic_ids[duplicate_indices] = np.random.choice(base_ids[:1000], len(duplicate_indices))\n",
    "\n",
    "    # NULLs (missing ID)\n",
    "    traffic_ids[np.random.choice(n_rows, int(n_rows * 0.005), replace=False)] = np.nan\n",
    "\n",
    "    # Int64 allows NA (pandas integer type for nullable columns)\n",
    "    df.insert(0, 'traffic_id', traffic_ids)\n",
    "\n",
    "    # --- Final Dtype Casting for correct NA handling ---\n",
    "    df['traffic_id'] = df['traffic_id'].astype('Int64')\n",
    "    df['vehicle_count'] = df['vehicle_count'].astype('Int64')\n",
    "    df['accident_count'] = df['accident_count'].astype('Int64')\n",
    "    df['visibility_m'] = df['visibility_m'].astype('Int64')\n",
    "\n",
    "    logging.info(\"Data generation complete.\")\n",
    "    return df\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Use the function to generate a DataFrame with the requested size\n",
    "DATA_SIZE = 5000 # Example size: 6500 rows\n",
    "traffic_df = generate_traffic_data(n_rows=DATA_SIZE)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "CSV_FILENAME = 'traffic_data.csv'\n",
    "# index=False prevents writing the pandas index as a column\n",
    "traffic_df.to_csv(CSV_FILENAME, index=False)\n",
    "print(f\"\\n--- Data Exported ---\")\n",
    "print(f\"DataFrame successfully saved to {CSV_FILENAME}\")\n",
    "\n",
    "# Print a summary to verify the generation and messiness\n",
    "print(\"\\n--- DataFrame Summary ---\")\n",
    "print(f\"Total Rows: {traffic_df.shape[0]}\")\n",
    "print(f\"Data Types:\\n{traffic_df.dtypes}\")\n",
    "print(\"\\nFirst 5 Rows (Note the various formats/NaNs):\")\n",
    "print(traffic_df.head())\n",
    "print(\"\\nValue Counts for a Messy Column (date_time, showing format variations):\")\n",
    "# Show the top non-null values to see format variations\n",
    "print(traffic_df['date_time'].value_counts(dropna=False).head(10))\n",
    "\n",
    "# Example of identifying messiness in vehicle_count (will show outliers and NaNs)\n",
    "print(\"\\nValue Counts for a Messy Column (vehicle_count, showing NaNs and potential outliers):\")\n",
    "print(traffic_df['vehicle_count'].value_counts(dropna=False).sort_index().tail(5))\n",
    "\n",
    "print(\"\\nValue Counts for traffic_id (showing duplicates and NaNs):\")\n",
    "print(traffic_df['traffic_id'].value_counts(dropna=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
